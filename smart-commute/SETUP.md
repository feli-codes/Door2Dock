# ğŸš² Smart-Commute â€“ GitHub Actions Setup

## So funktioniert es

GitHub Actions fÃ¼hrt alle **5 Minuten** einen Workflow aus.  
Jeder Workflow sammelt intern **5Ã— im Minutentakt** Daten â†’ effektiv **jede Minute** ein Datenpunkt.  
Die SQLite-Datenbank wird nach jedem Lauf zurÃ¼ck ins Repo committed.

```
Cron (alle 5 Min) â†’ python --burst â†’ Min 0, 1, 2, 3, 4 â†’ git commit DB â†’ fertig
```

---

## Einrichtung (10 Minuten)

### Schritt 1: Neues GitHub Repo erstellen

1. [github.com/new](https://github.com/new)
2. **Repository name**: `smart-commute`
3. **âš ï¸ Public** auswÃ¤hlen â†’ unbegrenzte Actions-Minuten!
4. **Haken setzen** bei "Add a README file"
5. **Create repository**

### Schritt 2: Dateien hochladen

1. Im Repo auf **Add file â†’ Upload files**
2. Lade hoch:
   - `bike_collector.py`
   - `requirements.txt`
   - `.gitignore` (Finder: `Cmd+Shift+.` um versteckte Dateien zu sehen)
3. **Commit changes**

### Schritt 3: Workflow-Datei erstellen

1. Im Repo auf **Add file â†’ Create new file**
2. Als Dateinamen **genau** eingeben: `.github/workflows/collect.yml`
   (GitHub erstellt die Ordner automatisch)
3. Inhalt der `collect.yml` Datei reinkopieren
4. **Commit changes**

### Schritt 4: Workflow-Berechtigung setzen

**âš ï¸ Das ist der wichtigste Schritt â€“ ohne das kann der Bot nicht committen!**

1. **Settings** â†’ **Actions** â†’ **General**
2. Runterscrollen zu **"Workflow permissions"**
3. **"Read and write permissions"** auswÃ¤hlen
4. **Save**

### Schritt 5: Testen

1. **Actions** Tab â†’ **"Collect Bike Data"** links auswÃ¤hlen
2. **"Run workflow"** â†’ **"Run workflow"** (grÃ¼ner Button)
3. Warte ~5 Minuten (5 Zyklen Ã  1 Minute)
4. Der Job sollte **grÃ¼n** âœ… werden
5. ZurÃ¼ck zu **Code** â†’ `data/commute.db` sollte erscheinen!

### Schritt 6: Fertig! ğŸ‰

Ab jetzt lÃ¤uft alles automatisch. Laptop aus, schlafen, egal.

---

## Daten herunterladen

### Option A: Direkt von GitHub
Im Repo â†’ `data/` â†’ `commute.db` â†’ **Download raw file**

### Option B: Per Terminal
```bash
git clone https://github.com/DEIN-USERNAME/smart-commute.git
cd smart-commute
python bike_collector.py --stats
```

### Option C: DB direkt abfragen
```bash
sqlite3 data/commute.db "SELECT COUNT(*) FROM bike_availability;"
sqlite3 data/commute.db "SELECT * FROM bike_availability ORDER BY timestamp DESC LIMIT 20;"
```

---

## Lokaler Betrieb (Alternative zu GitHub Actions)

Falls du das Skript auch lokal laufen lassen willst:

```bash
pip install requests

# Dauerbetrieb (jede Minute, Ctrl+C zum Stoppen)
python bike_collector.py

# Auf Mac: verhindert Sleep
caffeinate -i python bike_collector.py
```

---

## Troubleshooting

**Job lÃ¤uft, aber keine DB im Repo?**
â†’ Schritt 4 prÃ¼fen: Workflow permissions auf "Read and write" gesetzt?

**Job ist rot / Fehler?**
â†’ Actions Tab â†’ auf den Lauf klicken â†’ "collect" â†’ Logs lesen.  
  HÃ¤ufigste Ursache: TfL API kurz offline â†’ nÃ¤chster Lauf klappt meist.

**Cron triggert nicht pÃ¼nktlich?**
â†’ Normal. GitHub Actions hat bis zu 5-15 Min VerzÃ¶gerung bei Cron-Jobs.  
  Die Daten kommen trotzdem â€“ nur nicht sekundengenau.

**Wie stoppe ich die Sammlung?**
â†’ Actions Tab â†’ "Collect Bike Data" â†’ "â‹¯" oben rechts â†’ **Disable workflow**

---

## Datenmengen

| Zeitraum  | Datenpunkte (ca.)       | DB-GrÃ¶ÃŸe     |
|-----------|------------------------:|-------------:|
| 1 Tag     | ~12 Stationen Ã— 1440   | ~2 MB        |
| 1 Woche   | ~120.000                | ~15 MB       |
| 3 Wochen  | ~360.000                | ~45 MB       |

GitHub erlaubt Repos bis 1 GB â€“ kein Problem.
